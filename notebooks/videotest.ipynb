{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94e80f-bb53-471b-983d-d5b76dc78451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Understanding Example\n",
    "\n",
    "This notebook demonstrates the process of extracting frames from a video, processing these frames using a vision model, and generating textual descriptions using GPT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d22ef8-9412-4852-9580-117d15ecf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f43cb-b4df-45c8-b042-b7116247255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Extract Frames from the Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0ee51-16c1-460a-a505-56ec0aebeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder, interval=30):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        if count % interval == 0:\n",
    "            frame_path = os.path.join(output_folder, f\"frame_{count:03d}.jpg\")\n",
    "            cv2.imwrite(frame_path, image)\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "    vidcap.release()\n",
    "\n",
    "# Extract frames\n",
    "video_path = '../data/raw_videos/sample_video.mp4'\n",
    "output_folder = '../data/frames'\n",
    "extract_frames(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd48a6-2c34-43ac-8685-a50b272e77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Process Frames using a Vision Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6b658-0f87-49b6-aa3c-5810833fd9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frames(frames_folder, output_folder):\n",
    "    model, preprocess = clip.load(\"ViT-B/32\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for frame_name in os.listdir(frames_folder):\n",
    "        frame_path = os.path.join(frames_folder, frame_name)\n",
    "        image = Image.open(frame_path)\n",
    "        image_input = preprocess(image).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input)\n",
    "\n",
    "        features_path = os.path.join(output_folder, f\"{os.path.splitext(frame_name)[0]}_features.pt\")\n",
    "        torch.save(image_features, features_path)\n",
    "\n",
    "# Process frames\n",
    "frames_folder = '../data/frames'\n",
    "output_folder = '../data/processed_features'\n",
    "process_frames(frames_folder, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
